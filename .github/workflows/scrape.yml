name: Web Scraper

on:
  workflow_dispatch:
    inputs:
      urls:
        description: 'URLs to scrape (comma-separated)'
        required: true
        type: string
      output_branch:
        description: 'Branch to save scraped content'
        required: false
        default: 'scraped-content'
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Set up Chrome
        uses: browser-actions/setup-chrome@v1
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install aiohttp beautifulsoup4 selenium webdriver-manager
      
      - name: Create output directory
        run: mkdir -p pages
      
      - name: Run Web Scraper
        env:
          URLS: ${{ github.event.inputs.urls }}
        run: |
          # Split comma-separated URLs
          IFS=',' read -ra URL_ARRAY <<< "$URLS"
          
          # Run scraper with URLs
          python scraper.py "${URL_ARRAY[@]}"
      
      - name: Prepare HTML Archive
        run: |
          # Create an HTML file to archive the scraped content
          echo "<!DOCTYPE html>" > archive.html
          echo "<html lang='en'>" >> archive.html
          echo "<head><meta charset='UTF-8'><title>Scraped Content</title></head>" >> archive.html
          echo "<body><h1>Scraped Content</h1>" >> archive.html
          
          # Loop through the scraped pages and add them to the archive
          for file in pages/*.html; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              echo "<h2>${filename}</h2>" >> archive.html
              echo "<iframe src='$file' style='width:100%; height:600px; border:none;'></iframe>" >> archive.html
            fi
          done
          
          echo "</body></html>" >> archive.html
      
      - name: Check scraped content
        run: |
          echo "Scraped files:"
          ls -R pages/
          ls -R archive.html
      
      - name: Create scraped content branch
        run: |
          git config --global user.name 'GitHub Action'
          git config --global user.email 'action@github.com'
          
          # Create or switch to the output branch
          git checkout -B ${{ github.event.inputs.output_branch }}
          
          # Add scraped content and archive
          git add pages/
          git add archive.html
          
          # Commit changes
          git commit -m "Scraped web content at $(date)" || echo "No changes to commit"
          
          # Push to remote branch
          git push -f origin ${{ github.event.inputs.output_branch }}
      
      - name: Upload scraped content
        uses: actions/upload-artifact@v3
        with:
          name: scraped-sites
          path: |
            pages/
            archive.html
          retention-days: 7
