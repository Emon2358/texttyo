name: Scrape and Save Page

on:
  workflow_dispatch:
    inputs:
      target_url:
        description: 'スクレイピング対象のURL'
        required: true
        type: string

jobs:
  scrape-page:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            **/requirements.txt
            **/requirements-dev.txt

      - name: System information
        run: |
          python -V
          pip -V
          uname -a
          free -h
          df -h

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libgbm-dev \
            libatk1.0-0 \
            libatk-bridge2.0-0 \
            libcups2 \
            libdrm2 \
            libxkbcommon0 \
            libxcomposite1 \
            libxdamage1 \
            libxfixes3 \
            libxrandr2 \
            libgbm1 \
            libasound2t64 \
            libpango-1.0-0 \
            libpangocairo-1.0-0 \
            libnspr4 \
            libnss3 \
            libx11-xcb1 \
            libfontconfig1 \
            libfreetype6

      - name: Cache Playwright browsers
        uses: actions/cache@v3
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            playwright-${{ runner.os }}-

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || pip install playwright beautifulsoup4 asyncio urllib3 requests
          playwright install firefox --with-deps

      - name: Verify Playwright installation
        run: |
          playwright --version
          playwright install-deps firefox
          echo "import asyncio
          from playwright.async_api import async_playwright
          
          async def test_browser():
              async with async_playwright() as p:
                  browser = await p.firefox.launch()
                  await browser.close()
          
          asyncio.run(test_browser())" > test_playwright.py
          python test_playwright.py

      - name: Prepare output directory
        run: |
          mkdir -p sites
          touch url_patterns.json
          [ -s url_patterns.json ] || echo "{}" > url_patterns.json

      - name: Scrape and save page
        env:
          TARGET_URL: ${{ github.event.inputs.target_url }}
          PYTHONUNBUFFERED: "1"
          PLAYWRIGHT_BROWSERS_PATH: "0"
          DEBUG: "pw:api"
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: "0"
        run: |
          echo "スクレイピングを開始: $TARGET_URL"
          python scraper.py "$TARGET_URL"

      - name: Check scraping results
        run: |
          if [ -d "sites" ]; then
            echo "保存されたファイル:"
            find sites -type f -name "*.html" -exec ls -lh {} \;
          else
            echo "sitesディレクトリが見つかりません"
            exit 1
          fi

      - name: Commit and push changes
        if: success()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "GitHub Action"
          git config user.email "action@github.com"
          git add -f url_patterns.json sites/
          if [[ -n "$(git status --porcelain)" ]]; then
            git commit -m "Scrape: ${{ github.event.inputs.target_url }} ($(date -u '+%Y-%m-%d %H:%M:%S UTC'))"
            git push
          else
            echo "変更するファイルがありません"
          fi

      - name: Upload artifact on failure
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: debug-logs
          path: |
            ./*.log
            ./sites/
            ./url_patterns.json
          retention-days: 7
