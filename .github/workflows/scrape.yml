name: Scrape and Save Page

on:
  workflow_dispatch:
    inputs:
      target_url:
        description: 'スクレイピング対象のURL'
        required: true
        type: string

jobs:
  scrape-page:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            **/requirements.txt
            **/requirements-dev.txt

      - name: System information
        run: |
          python -V
          pip -V
          uname -a
          free -h
          df -h

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libgbm-dev \
            libatk1.0-0 \
            libatk-bridge2.0-0 \
            libcups2 \
            libdrm2 \
            libxkbcommon0 \
            libxcomposite1 \
            libxdamage1 \
            libxfixes3 \
            libxrandr2 \
            libgbm1 \
            libasound2 \
            libpango-1.0-0 \
            libpangocairo-1.0-0 \
            libnspr4 \
            libnss3 \
            libx11-xcb1 \
            libfontconfig1 \
            libfreetype6

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright beautifulsoup4 asyncio urllib3 requests

      - name: Install Playwright and browsers
        run: |
          playwright install
          playwright install-deps firefox
          playwright install firefox
          ls -la ~/.cache/ms-playwright/
          echo "PLAYWRIGHT_BROWSERS_PATH=$HOME/.cache/ms-playwright" >> $GITHUB_ENV

      - name: Verify Playwright installation
        run: |
          playwright --version
          python -c "
          import asyncio
          from playwright.async_api import async_playwright
          
          async def test_browser():
              async with async_playwright() as p:
                  browser = await p.firefox.launch(headless=True)
                  context = await browser.new_context()
                  page = await context.new_page()
                  await page.goto('about:blank')
                  await browser.close()
          
          asyncio.run(test_browser())
          "

      - name: Prepare output directory
        run: |
          mkdir -p sites
          touch url_patterns.json
          [ -s url_patterns.json ] || echo "{}" > url_patterns.json

      - name: Scrape and save page
        env:
          TARGET_URL: ${{ github.event.inputs.target_url }}
          PYTHONUNBUFFERED: "1"
          DEBUG: "pw:api"
        run: |
          echo "スクレイピングを開始: $TARGET_URL"
          python scraper.py "$TARGET_URL"

      - name: Check scraping results
        run: |
          if [ -d "sites" ]; then
            echo "保存されたファイル:"
            find sites -type f -name "*.html" -exec ls -lh {} \;
          else
            echo "sitesディレクトリが見つかりません"
            exit 1
          fi

      - name: Commit and push changes
        if: success()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "GitHub Action"
          git config user.email "action@github.com"
          git add -f url_patterns.json sites/
          if [[ -n "$(git status --porcelain)" ]]; then
            git commit -m "Scrape: ${{ github.event.inputs.target_url }} ($(date -u '+%Y-%m-%d %H:%M:%S UTC'))"
            git push
          else
            echo "変更するファイルがありません"
          fi

      - name: Upload artifact on failure
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: debug-logs
          path: |
            ./*.log
            ./sites/
            ./url_patterns.json
          retention-days: 7
