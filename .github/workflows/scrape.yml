name: Web Scraper

on:
  workflow_dispatch:
    inputs:
      urls:
        description: 'URLs to scrape (comma-separated)'
        required: true
        type: string
      output_branch:
        description: 'Branch to save scraped content'
        required: false
        default: 'scraped-content'
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          # 全ブランチ履歴를 가져오기
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Set up Chrome
        uses: browser-actions/setup-chrome@v1
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install aiohttp beautifulsoup4 selenium webdriver-manager
      
      - name: Create output directories
        run: |
          mkdir -p scraped_content/pages
          mkdir -p scraped_content/metadata
      
      - name: Run Web Scraper
        env:
          URLS: ${{ github.event.inputs.urls }}
        run: |
          # Split comma-separated URLs
          IFS=',' read -ra URL_ARRAY <<< "$URLS"
          
          # Run scraper with URLs and specify output directory
          python scraper.py "${URL_ARRAY[@]}" scraped_content/pages
      
      - name: Prepare HTML Archive
        run: |
          # Create main index file
          echo "<!DOCTYPE html>" > scraped_content/index.html
          echo "<html lang='en'>" >> scraped_content/index.html
          echo "<head><meta charset='UTF-8'><title>Scraped Content Archive</title></head>" >> scraped_content/index.html
          echo "<body><h1>Scraped Content Archive</h1><ul>" >> scraped_content/index.html
          
          # Loop through scraped pages
          for file in scraped_content/pages/*.html; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              echo "<li><a href='pages/$filename'>$filename</a></li>" >> scraped_content/index.html
            fi
          done
          
          echo "</ul></body></html>" >> scraped_content/index.html
      
      - name: Create Metadata Summary
        run: |
          echo "# Scraping Metadata" > scraped_content/METADATA.md
          echo "" >> scraped_content/METADATA.md
          echo "## Scraped URLs" >> scraped_content/METADATA.md
          echo "\`\`\`" >> scraped_content/METADATA.md
          echo "${{ github.event.inputs.urls }}" >> scraped_content/METADATA.md
          echo "\`\`\`" >> scraped_content/METADATA.md
          echo "" >> scraped_content/METADATA.md
          echo "## Scraping Details" >> scraped_content/METADATA.md
          echo "- Date: $(date)" >> scraped_content/METADATA.md
          echo "- Workflow Run ID: ${{ github.run_id }}" >> scraped_content/METADATA.md
      
      - name: Configure Git
        run: |
          git config --global user.name 'GitHub Action'
          git config --global user.email 'action@github.com'
      
      - name: Create and Push to Scraped Content Branch
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Create a new branch or switch to existing branch
          git checkout -B ${{ github.event.inputs.output_branch }}
          
          # Add all scraped content
          git add scraped_content/
          
          # Commit changes
          git commit -m "Scrape web content at $(date)" -a || echo "No changes to commit"
          
          # Force push to remote branch
          git push -f origin ${{ github.event.inputs.output_branch }}
      
      - name: Upload Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: scraped-content
          path: scraped_content/
          retention-days: 7
